{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EQWorks Solution\n",
    " \n",
    "Author: Tryambak Kaushik\n",
    "\n",
    "Date: 07 February 2021\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Row, Window\n",
    "\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a SparkContext and SparkSession\n",
    "sc = SparkContext(\"local\",\"firstapp\")\n",
    "# sc.stop()\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"EQ Soln\")\\\n",
    ".config(\"spark.some.config.option\", \"some-value\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _ID: string (nullable = true)\n",
      " |--  TimeSt: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Province: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data/DataSample.csv to Spark DataFrame\n",
    "\n",
    "df_dataSample = spark.read.option(\"header\",True).csv(\"data\\DataSample.csv\")\n",
    "\n",
    "print('Display Schema of DataSample.csv dataset table')\n",
    "df_dataSample.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-------+--------+------------+--------+---------+\n",
      "|    _ID|              TimeSt|Country|Province|        City|Latitude|Longitude|\n",
      "+-------+--------------------+-------+--------+------------+--------+---------+\n",
      "|4516516|2017-06-21 00:00:...|     CA|      ON|    Waterloo|43.49347|-80.49123|\n",
      "|4516547|2017-06-21 18:00:...|     CA|      ON|      London|42.93990|-81.27090|\n",
      "|4516550|2017-06-21 15:00:...|     CA|      ON|      Guelph|43.57760|-80.22010|\n",
      "|4516600|2017-06-21 15:00:...|     CA|      ON|   Stratford|43.37160|-80.97730|\n",
      "|4516613|2017-06-21 15:00:...|     CA|      ON|   Stratford|43.37160|-80.97730|\n",
      "|4516693|2017-06-21 14:00:...|     CA|      ON|   Kitchener|43.43810|-80.50990|\n",
      "|4516771|2017-06-21 10:00:...|     CA|      ON|      Sarnia|42.96100|-82.37300|\n",
      "|4516831|2017-06-21 12:00:...|     CA|      ON|      London|43.00910|-81.17650|\n",
      "|4516915|2017-06-21 15:00:...|     CA|      ON|      London|43.00910|-81.17650|\n",
      "|4516953|2017-06-21 16:00:...|     CA|      ON|   Kitchener|43.42780|-80.51350|\n",
      "|4516966|2017-06-21 01:00:...|     CA|      ON|   Kitchener|43.43810|-80.50990|\n",
      "|4517044|2017-06-21 21:00:...|     CA|      ON|     Windsor|42.28250|-83.03720|\n",
      "|4517047|2017-06-21 12:00:...|     CA|      ON|      London|43.00910|-81.17650|\n",
      "|4517081|2017-06-21 22:00:...|     CA|      ON|     Windsor|42.24397|-82.98058|\n",
      "|4517175|2017-06-21 19:00:...|     CA|      ON|Saint Thomas|42.77920|-81.19270|\n",
      "|4517220|2017-06-21 18:00:...|     CA|      ON|    Waterloo|43.46340|-80.52010|\n",
      "|4517250|2017-06-21 03:00:...|     CA|      ON|     Chatham|42.42470|-82.17550|\n",
      "|4517303|2017-06-21 03:00:...|     CA|      ON|     Windsor|42.29570|-82.95990|\n",
      "|4517326|2017-06-21 04:00:...|     CA|      ON|      Guelph|43.54250|-80.26760|\n",
      "|4517347|2017-06-21 03:00:...|     CA|      ON|     Windsor|42.29570|-82.95990|\n",
      "+-------+--------------------+-------+--------+------------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Display the contents of DataSample data\n",
    "print('Display contents of DataSample.csv dataset table')\n",
    "df_dataSample.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Cleanup\n",
    "\n",
    "A sample dataset of request logs is given in data/DataSample.csv. We consider records that have identical geoinfo and timest as suspicious. Please clean up the sample dataset by filtering out those suspicious request records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-------+--------+-------------+--------+----------+\n",
      "|    _ID|              TimeSt|Country|Province|         City|Latitude| Longitude|\n",
      "+-------+--------------------+-------+--------+-------------+--------+----------+\n",
      "|4516516|2017-06-21 00:00:...|     CA|      ON|     Waterloo|43.49347| -80.49123|\n",
      "|4519209|2017-06-21 00:00:...|     CA|      ON|      Hanover|44.15170| -81.02660|\n",
      "|4518130|2017-06-21 00:00:...|     CA|      ON|       London|43.00040| -81.23430|\n",
      "|4521574|2017-06-21 00:00:...|     CA|      ON|    Brantford|43.15080| -80.20940|\n",
      "|4524947|2017-06-21 00:00:...|     CA|      ON|    Kitchener|43.43060| -80.48770|\n",
      "|4530820|2017-06-21 00:00:...|     CA|      NB|      Moncton|46.11830| -64.73380|\n",
      "|4534383|2017-06-21 00:00:...|     CA|      AB|Fort Mcmurray|56.74308|-111.47651|\n",
      "|5380915|2017-06-21 00:01:...|     CA|      AB|      Calgary|51.15880|-113.96360|\n",
      "|4536827|2017-06-21 00:01:...|     CA|      ON|       Oshawa|43.90635| -78.87251|\n",
      "|5387037|2017-06-21 00:01:...|     CA|      QC|  Chibougamau|49.91680| -74.36590|\n",
      "|4559896|2017-06-21 00:01:...|     CA|      ON|      Timmins|48.54390| -81.11840|\n",
      "|4567776|2017-06-21 00:01:...|     CA|      ON|      Toronto|43.64560| -79.37540|\n",
      "|4573627|2017-06-21 00:01:...|     CA|      AB|       Ponoka|52.67450|-113.57600|\n",
      "|4573118|2017-06-21 00:01:...|     CA|      BC|       Surrey|49.03740|-122.82990|\n",
      "|4575139|2017-06-21 00:01:...|     CA|      AB|     Red Deer|52.26940|-113.76900|\n",
      "|4576375|2017-06-21 00:02:...|     CA|      QC|     Gatineau|45.50025| -75.65488|\n",
      "|5392555|2017-06-21 00:02:...|     CA|      ON|      Toronto|43.69861| -79.39311|\n",
      "|5394149|2017-06-21 00:02:...|     CA|      QC|Saint-Nicolas|46.71258| -71.29079|\n",
      "|4581069|2017-06-21 00:02:...|     CA|      MB|     Winnipeg|49.88400| -97.16500|\n",
      "|4581672|2017-06-21 00:02:...|     CA|      ON|      Toronto|43.69600| -79.41200|\n",
      "+-------+--------------------+-------+--------+-------------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop duplicate rows based on columns TimeSt, Latitude and Longitude\n",
    "\n",
    "df_clean = df_dataSample.dropDuplicates(['Latitude', 'Longitude']).dropDuplicates([' TimeSt'])\n",
    "\n",
    "print ('Display clean dataset after dropping suspicius requests (i.e., duplicate geoinfo and timest)')\n",
    "df_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------END OF ANSWER #1------------------------\n"
     ]
    }
   ],
   "source": [
    "print (\"------------------------END OF ANSWER #1------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End of Answer #1**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Label\n",
    "Assign each request (from data/DataSample.csv) to the closest (i.e. minimum distance) POI (from data/POIList.csv).\n",
    "\n",
    "**Note:** A POI is a geographical Point of Interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Display Schema and data of POIList\n",
      "\n",
      "root\n",
      " |-- POIID: string (nullable = true)\n",
      " |--  Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      "\n",
      "+-----+----------+------------+\n",
      "|POIID|  Latitude|   Longitude|\n",
      "+-----+----------+------------+\n",
      "| POI1| 53.546167| -113.485734|\n",
      "| POI2| 53.546167| -113.485734|\n",
      "| POI3| 45.521629|  -73.566024|\n",
      "| POI4| 45.224830|  -63.232729|\n",
      "+-----+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Load data from data/POIList.csv in Spark Dataframe\n",
    "\n",
    "df_poil = spark.read.option(\"header\",True).csv(\"data\\POIList.csv\")\n",
    "\n",
    "print ('\\nDisplay Schema and data of POIList dataset table\\n')\n",
    "df_poil.printSchema()\n",
    "df_poil.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POIID</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POI1</td>\n",
       "      <td>53.546167</td>\n",
       "      <td>-113.485734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POI2</td>\n",
       "      <td>53.546167</td>\n",
       "      <td>-113.485734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>POI3</td>\n",
       "      <td>45.521629</td>\n",
       "      <td>-73.566024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>POI4</td>\n",
       "      <td>45.224830</td>\n",
       "      <td>-63.232729</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  POIID    Latitude     Longitude\n",
       "0  POI1   53.546167   -113.485734\n",
       "1  POI2   53.546167   -113.485734\n",
       "2  POI3   45.521629    -73.566024\n",
       "3  POI4   45.224830    -63.232729"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert pois Spark DataFrame to Pandas Dataframe\n",
    "df_pd_pois = df_poil.toPandas()\n",
    "df_pd_pois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Display the dataframe with new columns of nearest POI and POI_DIS(i.e, distance to POI from request)\n",
      "+-------+--------------------+-------+--------+---------+--------+---------+----+--------+\n",
      "|    _ID|              TimeSt|Country|Province|     City|Latitude|Longitude| POI| POI_DIS|\n",
      "+-------+--------------------+-------+--------+---------+--------+---------+----+--------+\n",
      "|4516516|2017-06-21 00:00:...|     CA|      ON| Waterloo|43.49347|-80.49123|POI3|7.216083|\n",
      "|4519209|2017-06-21 00:00:...|     CA|      ON|  Hanover|44.15170|-81.02660|POI3|7.585312|\n",
      "|4518130|2017-06-21 00:00:...|     CA|      ON|   London|43.00040|-81.23430|POI3|8.072114|\n",
      "|4521574|2017-06-21 00:00:...|     CA|      ON|Brantford|43.15080|-80.20940|POI3|7.053739|\n",
      "|4524947|2017-06-21 00:00:...|     CA|      ON|Kitchener|43.43060|-80.48770|POI3|7.230631|\n",
      "+-------+--------------------+-------+--------+---------+--------+---------+----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Python-UDF to find POI with minimum distance to each entry of DataSample\n",
    "def myfun(la2, lo2):\n",
    "    \n",
    "    min_dis = 1.0e10\n",
    "    poi_id = df_pd_pois.loc[0,'POIID']\n",
    "    \n",
    "    for i, (la1,lo1) in enumerate( zip(df_pd_pois[' Latitude'], df_pd_pois['Longitude'])):\n",
    "        la1, lo1 = float(la1), float(lo1)\n",
    "        dis = math.sqrt((la1-la2)**2 + (lo1-lo2)**2)\n",
    "        if min_dis > dis:\n",
    "            min_dis = dis\n",
    "            poi_id = df_pd_pois.loc[i,'POIID']\n",
    "            \n",
    "    return ([poi_id, min_dis])\n",
    "\n",
    "#Register Python-UDF with Spark-UDF\n",
    "myfun_spark = F.udf(myfun, ArrayType(StringType()))\n",
    "\n",
    "df_poi = df_clean.withColumn('temp_col', myfun_spark(  F.col('Latitude').cast(FloatType()),\n",
    "                                          F.col('Longitude').cast(FloatType())  )).cache()\\\n",
    "            .withColumn('POI', F.col('temp_col')[0])\\\n",
    "            .withColumn('POI_DIS', F.col('temp_col')[1].cast(DoubleType()))\\\n",
    "            .drop('temp_col')\n",
    "\n",
    "print('Display the dataframe with new columns of nearest POI and POI_DIS(i.e, distance to POI from request)')\n",
    "df_poi.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------END OF ANSWER #2------------------------\n"
     ]
    }
   ],
   "source": [
    "print (\"------------------------END OF ANSWER #2------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End of Answer #2**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Analysis\n",
    "For each POI, calculate the average and standard deviation of the distance between the POI to each of its assigned requests.\n",
    "\n",
    "At each POI, draw a circle (with the center at the POI) that includes all of its assigned requests. Calculate the radius and density (requests/area) for each POI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Display distance Average and Std_Dev for each POI\n",
      "+-----+----------+------------+------------------+-----------------+\n",
      "|POIID|  Latitude|   Longitude|           Average|          Std_Dev|\n",
      "+-----+----------+------------+------------------+-----------------+\n",
      "| POI1| 53.546167| -113.485734| 4.548856149843565|4.666481513614549|\n",
      "| POI2| 53.546167| -113.485734|              null|             null|\n",
      "| POI3| 45.521629|  -73.566024| 5.356265976821089| 3.04113827053302|\n",
      "| POI4| 45.224830|  -63.232729|12.736570363332119|35.65542795879107|\n",
      "+-----+----------+------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Group the dataframe df_poi on 'POI' column and calculate average and standard deviation on each group\n",
    "df_avgSD = df_poi.groupby('POI').agg(F.avg('POI_DIS').alias('Average'), F.stddev('POI_DIS').alias('Std_Dev'))\n",
    "\n",
    "#Left Join df_avgSD dataframe to df_poil dataframe for completeness\n",
    "df_avgSD = df_poil.join(df_avgSD, df_poil.POIID == df_avgSD.POI, how = 'Left').drop(df_avgSD.POI)\n",
    "\n",
    "print('Display distance Average and Std_Dev for each POI')\n",
    "df_avgSD.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Based on above output, it can be concluded that POI2 radius of influence is ZERO\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (\"Note: Based on above output, it can be concluded that POI2 radius of influence is ZERO\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Based on above output, it can be concluded that POI2 radius of influence is ZERO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Display the maximum POI_DIS (i.e, POI_RADIUS) values for each group\n",
      "\n",
      "+-----+----------+------------+------------------+-----------------+----------+\n",
      "|POIID|  Latitude|   Longitude|           Average|          Std_Dev|POI_RADIUS|\n",
      "+-----+----------+------------+------------------+-----------------+----------+\n",
      "| POI1| 53.546167| -113.485734| 4.548856149843565|4.666481513614549| 24.851934|\n",
      "| POI2| 53.546167| -113.485734|              null|             null|      null|\n",
      "| POI3| 45.521629|  -73.566024| 5.356265976821089| 3.04113827053302| 20.155376|\n",
      "| POI4| 45.224830|  -63.232729|12.736570363332119|35.65542795879107| 192.70499|\n",
      "+-----+----------+------------+------------------+-----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#The radius of Influence-Circle of POI will be the distance to farthest assigned request\n",
    "\n",
    "w = Window.partitionBy('POI')\n",
    "\n",
    "df_radius = df_poi.withColumn('max_r', F.max('POI_DIS').over(w))\\\n",
    "                  .where(F.col('POI_DIS') == F.col('max_r'))\\\n",
    "                  .drop('max_r')\n",
    "\n",
    "#Left Join df_radius dataframe to df_poil dataframe for completeness\n",
    "df_avgSD_r = df_avgSD.join(df_radius['POI', 'POI_DIS'], df_avgSD.POIID == df_radius.POI, how = 'Left')\\\n",
    "                   .drop(df_radius.POI)\\\n",
    "                   .withColumnRenamed('POI_DIS', 'POI_RADIUS')\n",
    "\n",
    "print('Display the maximum POI_DIS (i.e, POI_RADIUS) values for each group\\n')\n",
    "df_avgSD_r.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dislay No. of Requests and Density for each POI\n",
      "+-----+----------+------------+------------------+-----------------+----------+--------+--------------------+\n",
      "|POIID|  Latitude|   Longitude|           Average|          Std_Dev|POI_RADIUS|Requests|             Density|\n",
      "+-----+----------+------------+------------------+-----------------+----------+--------+--------------------+\n",
      "| POI1| 53.546167| -113.485734| 4.548856149843565|4.666481513614549| 24.851934|    3027|  1.5608543339196206|\n",
      "| POI2| 53.546167| -113.485734|              null|             null|      null|    null|                null|\n",
      "| POI3| 45.521629|  -73.566024| 5.356265976821089| 3.04113827053302| 20.155376|    2297|  1.8007338135858355|\n",
      "| POI4| 45.224830|  -63.232729|12.736570363332119|35.65542795879107| 192.70499|     224|0.001921022556875...|\n",
      "+-----+----------+------------+------------------+-----------------+----------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Calculate number of requests for each POI\n",
    "df_no_of_req = df_poi.groupby('POI').agg(F.count('POI').alias('Requests'))\n",
    "\n",
    "#Append POI_No.\n",
    "df_poi_req = df_avgSD_r.join(df_no_of_req, df_avgSD_r.POIID == df_no_of_req.POI, 'Left' )\\\n",
    "                         .drop(df_no_of_req['POI'])\n",
    "\n",
    "#Calculate the density\n",
    "df_poi_density = df_poi_req.withColumn('Density', F.col('Requests')/ (3.14*F.col('POI_RADIUS')**2 ))\n",
    "\n",
    "print('Dislay No. of Requests and Density for each POI')\n",
    "df_poi_density.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------END OF ANSWER #3------------------------\n"
     ]
    }
   ],
   "source": [
    "print (\"------------------------END OF ANSWER #3------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End of Answer #3**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data Science/Engineering Tracks\n",
    "Please complete either 4a or 4b. Extra points will be awarded for completing both tasks.\n",
    "\n",
    "#### 4a. Model\n",
    "To visualize the popularity of each POI, they need to be mapped to a scale that ranges from -10 to 10. Please provide a mathematical model to implement this, taking into consideration of extreme cases and outliers. Aim to be more sensitive around the average and provide as much visual differentiability as possible.\n",
    "Bonus: Try to come up with some reasonable hypotheses regarding POIs, state all assumptions, testing steps and conclusions. Include this as a text file (with a name bonus) in your final submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import PySpark Libraries for Data Analytics\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------------+------------------+-----------------+----------+--------+--------------------+\n",
      "|POIID|  Latitude|   Longitude|           Average|          Std_Dev|POI_RADIUS|Requests|             Density|\n",
      "+-----+----------+------------+------------------+-----------------+----------+--------+--------------------+\n",
      "| POI1| 53.546167| -113.485734| 4.548856149843565|4.666481513614549| 24.851934|    3027|  1.5608543339196206|\n",
      "| POI2| 53.546167| -113.485734|              null|             null|      null|    null|                null|\n",
      "| POI3| 45.521629|  -73.566024| 5.356265976821089| 3.04113827053302| 20.155376|    2297|  1.8007338135858355|\n",
      "| POI4| 45.224830|  -63.232729|12.736570363332119|35.65542795879107| 192.70499|     224|0.001921022556875...|\n",
      "+-----+----------+------------+------------------+-----------------+----------+--------+--------------------+\n",
      "\n",
      "+-----+----------+------------+------------------+-----------------+----------+--------+--------------------+\n",
      "|POIID|  Latitude|   Longitude|           Average|          Std_Dev|POI_RADIUS|Requests|             Density|\n",
      "+-----+----------+------------+------------------+-----------------+----------+--------+--------------------+\n",
      "| POI1| 53.546167| -113.485734| 4.548856149843565|4.666481513614549| 24.851934|    3027|  1.5608543339196206|\n",
      "| POI3| 45.521629|  -73.566024| 5.356265976821089| 3.04113827053302| 20.155376|    2297|  1.8007338135858355|\n",
      "| POI4| 45.224830|  -63.232729|12.736570363332119|35.65542795879107| 192.70499|     224|0.001921022556875...|\n",
      "+-----+----------+------------+------------------+-----------------+----------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_poi_density.show()\n",
    "df_poi_density_temp = df_poi_density.filter(df_poi_density.Density.isNotNull())\n",
    "df_poi_density_temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Display scaled density for each POI\n",
      "+-----+---------+---------+-------+-------+----------+--------+-------+--------------+\n",
      "|POIID| Latitude|Longitude|Average|Std_Dev|POI_RADIUS|Requests|Density|Density_Scaled|\n",
      "+-----+---------+---------+-------+-------+----------+--------+-------+--------------+\n",
      "| POI1|   53.546| -113.486|  4.549|  4.666|    24.852|    3027|  1.561|         7.333|\n",
      "| POI3|   45.522|  -73.566|  5.356|  3.041|    20.155|    2297|  1.801|          10.0|\n",
      "| POI4|   45.225|  -63.233| 12.737| 35.655|   192.705|     224|  0.002|         -10.0|\n",
      "+-----+---------+---------+-------+-------+----------+--------+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Spark-udf for converting column from vector type to double type\n",
    "myfun_vec2double = F.udf(lambda x: round(float(list(x)[0]),3), DoubleType())\n",
    "\n",
    "# Use Spark VectorAssembler Transformation - Converting column to vector type\n",
    "assembler = VectorAssembler(inputCols=['Density'],outputCol=\"Density_Vector\")\n",
    "\n",
    "# Use Spark MinMaxScaler Transformation to scale the column within (min,max) range\n",
    "scaler = MinMaxScaler(min = -10, max = 10, inputCol=\"Density_Vector\", outputCol=\"Density_Scaled\")\n",
    "\n",
    "# Create a Spark Pipeline of VectorAssembler and MinMaxScaler\n",
    "pipeline = Pipeline(stages=[assembler, scaler])\n",
    "\n",
    "#Drop POI2 as outlier \n",
    "df_poi_density_temp = df_poi_density.filter(df_poi_density.Density.isNotNull())\n",
    "\n",
    "# Spark fitting pipeline on dataframe\n",
    "df_norm = pipeline.fit(df_poi_density_temp).transform(df_poi_density_temp)\\\n",
    "                  .withColumn(\"Density_Scaled\", myfun_vec2double(\"Density_Scaled\"))\\\n",
    "                  .drop(\"Density_Vector\")\n",
    "\n",
    "print('Display scaled density for each POI')\n",
    "df_norm.select(*['POIID'], *[F.round(c, 3).alias(c) for c in df_norm.columns[1:] ]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Display scaled density for each POI\n",
      "+-----+---------+---------+-------+-------+----------+--------+-------+--------------+-----------+------------------+\n",
      "|POIID| Latitude|Longitude|Average|Std_Dev|POI_RADIUS|Requests|Density|Density_Scaled|log_Density|log_Density_Scaled|\n",
      "+-----+---------+---------+-------+-------+----------+--------+-------+--------------+-----------+------------------+\n",
      "| POI1|   53.546| -113.486|  4.549|  4.666|    24.852|    3027|  1.561|         7.333|      0.193|             0.958|\n",
      "| POI3|   45.522|  -73.566|  5.356|  3.041|    20.155|    2297|  1.801|          10.0|      0.255|               1.0|\n",
      "| POI4|   45.225|  -63.233| 12.737| 35.655|   192.705|     224|  0.002|         -10.0|     -2.716|              -1.0|\n",
      "+-----+---------+---------+-------+-------+----------+--------+-------+--------------+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_lognorm = df_norm.withColumn('log_Density', F.log10(F.col('Density')) )\n",
    "\n",
    "# Use Spark VectorAssembler Transformation - Converting column to vector type\n",
    "assembler_log = VectorAssembler(inputCols=['log_Density'],outputCol=\"log_Density_Vector\")\n",
    "\n",
    "# Use Spark MinMaxScaler Transformation to scale the column within (min,max) range\n",
    "scaler_log = MinMaxScaler(min = -1.0, max = 1.0, inputCol=\"log_Density_Vector\", outputCol=\"log_Density_Scaled\")\n",
    "\n",
    "# Create a Spark Pipeline of VectorAssembler and MinMaxScaler\n",
    "pipeline_log = Pipeline(stages=[assembler_log, scaler_log])\n",
    "\n",
    "\n",
    "# Spark fitting pipeline on dataframe\n",
    "df_lognorm = pipeline_log.fit(df_lognorm).transform(df_lognorm)\\\n",
    "                  .withColumn(\"log_Density_Scaled\", myfun_vec2double(\"log_Density_Scaled\"))\\\n",
    "                  .drop(\"log_Density_Vector\")\n",
    "\n",
    "print('Display scaled density for each POI')\n",
    "df_lognorm.select(*['POIID'], *[F.round(c, 3).alias(c) for c in df_lognorm.columns[1:] ]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the interpretation on results in 'bonus' file\n",
    "bonus = \"\"\"\n",
    "Interpretation:\n",
    "Density column is the ratio of Requests to POI_Area. log_Density was calculated by taking log10 of Density values. log_Density were scaled in range (-10,10) to calculate log_Density_Scaled.\n",
    "\n",
    "It is difficult to come up with a statitics with only 3 good POIs.\n",
    "\n",
    "Nonetheless, the density values of POI1 and POI3 are 3 orders higher than POI4. Hence, Density_Scaled, log_Density and log_Density_Scaled values are also skewed.\n",
    "POI1 and POI3 attract more customers or requests per unit area of influence.\n",
    "\n",
    "Assumptions: POI2 was dropped as outlier. POI2 data must be investigated to identify the cause of zero zone of influence. Bad data collection and formatting can be reasons for POI2 being outlier\n",
    "\"\"\"\n",
    "\n",
    "with open('bonus', 'w') as f:\n",
    "    f.write(bonus)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus: Try to come up with some reasonable hypotheses regarding POIs, state all assumptions, testing steps and conclusions. Include this as a text file (with a name bonus) in your final submission.\n",
    "\n",
    "**Interpretation:**\n",
    "Density column is the ratio of Requests to POI_Area. log_Density was calculated by taking log10 of Density values. log_Density were scaled in range (-10,10) to calculate log_Density_Scaled.\n",
    "\n",
    "It is difficult to come up with a statitics with only 3 good POIs.\n",
    "\n",
    "Nonetheless, the density values of POI1 and POI3 are 3 orders higher than POI4. Hence, Density_Scaled, log_Density and log_Density_Scaled values are also skewed.\n",
    "POI1 and POI3 attract more customers or requests per unit area of influence.\n",
    "\n",
    "**Assumptions:** POI2 was dropped as outlier. POI2 data must be investigated to identify the cause of zero zone of influence. Bad data collection and formatting can be reasons for POI2 being outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------END OF ANSWER #4a------------------------\n"
     ]
    }
   ],
   "source": [
    "print (\"------------------------END OF ANSWER #4a------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End of Answer #4a**\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4b. Pipeline Dependency\n",
    "We use a modular design on all of our data analysis tasks. To get to a final product, we organize steps using a data pipeline. One task may require the output of one or multiple other tasks to run successfully. This creates dependencies between tasks.\n",
    "\n",
    "We also require the pipeline to be flexible. This means a new task may enter a running pipeline anytime that may not have the tasks' dependencies satisfied. In this event, we may have a set of tasks already running or completed in the pipeline, and we will need to map out which tasks are prerequisites for the newest task so the pipeline can execute them in the correct order. For optimal pipeline execution, when we map out the necessary tasks required to execute the new task, we want to avoid scheduling tasks that have already been executed.\n",
    "\n",
    "If we treat each task as a node and the dependencies between a pair of tasks as directed edges, we can construct a DAG (Wiki: Directed Acyclic Graph).\n",
    "\n",
    "Consider the following scenario. At a certain stage of our data processing, we have a set of tasks (starting tasks) that we know all its prerequisite task has been executed, and we wish to reach to a later goal task. We need to map out a path that indicates the order of executions on tasks that finally leads to the goal task. We are looking for a solution that satisfies both necessity and sufficiency -- if a task is not a prerequisite task of goal, or its task is a prerequisite task for starting tasks (already been executed), then it shouldn't be included in the path. The path needs to follow a correct topological ordering of the DAG, hence a task needs to be placed behind all its necessary prerequisite tasks in the path.\n",
    "\n",
    "Note: A starting task should be included in the path, if and only if it's a prerequisite of the goal task\n",
    "\n",
    "For example, we have 6 tasks [A, B, C, D, E, F], C depends on A (denoted as A->C), B->C, C->E, E->F. A new job has at least 2 tasks and at most 6 tasks, each task can only appear once.\n",
    "\n",
    "Examples:\n",
    "\n",
    "Inputs: starting task: A, goal task: F, output: A,B,C,E,F or B,A,C,E,F.\n",
    "Input: starting task: A,C, goal task:'F', outputs: C,E,F.\n",
    "You will find the starting task and the goal task in question.txt file, list of all tasks in task_ids.txt and dependencies in relations.txt.\n",
    "\n",
    "Please submit your implementation and result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'starting task': '73', 'goal task': '36'}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Assign questions data\n",
    "questions = {'starting task': '73', 'goal task': '36'}\n",
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign relations data\n",
    "relations = [(97,102),\n",
    "             (75,31),\n",
    "             (75,37),\n",
    "             (100,20),\n",
    "             (102,36),\n",
    "             (102,37),\n",
    "             (102,31),\n",
    "             (16,37),\n",
    "             (39,73),\n",
    "             (39,100),\n",
    "             (41,73),\n",
    "             (41,112),\n",
    "             (62,55),\n",
    "             (112,97),\n",
    "             (20,94),\n",
    "             (20,97),\n",
    "             (21,20),\n",
    "             (73,20),\n",
    "             (56,102),\n",
    "             (56,75),\n",
    "             (56,55),\n",
    "             (55,31),\n",
    "             (55,37),\n",
    "             (94,56),\n",
    "             (94,102)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign Task-IDs data\n",
    "task_ids = [97,75,100,102,16,39,41,62,112,20,21,73,56,55,36,37,94,31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>97</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>75</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   from   to\n",
       "0    97  102\n",
       "1    75   31\n",
       "2    75   37\n",
       "3   100   20\n",
       "4   102   36"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a pandas-dataframe of relations data\n",
    "r_pd = pd.DataFrame(relations, columns = ['from', 'to'])\n",
    "r_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Task: 73\n",
      "Goal Task: 36\n"
     ]
    }
   ],
   "source": [
    "#Get starting target (st) and goal target (gt)\n",
    "st = int(questions['starting task']); print ('Starting Task: %2d'%(st))\n",
    "gt = int(questions['goal task']); print ('Goal Task: %2d'%(gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The different paths from Starting Target to Goal Target\n",
      "\n",
      "[[73, 20, 94, 56, 102, 36], [73, 20, 94, 102, 36], [73, 20, 97, 102, 36]]\n"
     ]
    }
   ],
   "source": [
    "#A python recursive function to find the path from source to target\n",
    "def replicate_recur(st, gt, mylist=None):\n",
    "\n",
    "    # If a list has not been passed as argument create an empty one\n",
    "    if(mylist == None):\n",
    "        mylist = [st]\n",
    "        \n",
    "    if st == gt:\n",
    "        return mylist\n",
    "    \n",
    "    temp = r_pd[r_pd['from'] == st].values\n",
    "\n",
    "    if not temp.any() :\n",
    "        temp = 'Error'\n",
    "        mylist.append(temp)\n",
    "        return mylist\n",
    "    \n",
    "    mylist = [ [i for i in mylist] for _ in range(len(temp))]\n",
    "    for idx,val in enumerate(temp[:,1]):\n",
    "        mylist[idx].append(val)\n",
    "        mylist[idx] = replicate_recur(val, gt, mylist[idx])\n",
    "\n",
    "    return mylist\n",
    "\n",
    "output = []\n",
    "def removeNestings(l): \n",
    "    for i in l: \n",
    "        if (type(i) == list) & (type(i[0]) == list):\n",
    "            removeNestings(i) \n",
    "        elif ('Error' not in i):\n",
    "            output.append(i)\n",
    "\n",
    "print ('The different paths from Starting Target to Goal Target\\n')\n",
    "removeNestings([replicate_recur(st, gt)])\n",
    "pprint(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------END OF ANSWER #4b------------------------\n"
     ]
    }
   ],
   "source": [
    "print (\"------------------------END OF ANSWER #4b------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
